# AI 기반 커뮤니티 핫딜 분석 시스템

커뮤니티 핫딜 게시판을 크롤링하여 불필요한 항목을 필터링하고, AI를 활용해 회사 비품 구매에 적합한 "생필품(Daily Necessities)" 핫딜을 자동으로 식별하는 시스템입니다.

## 1. 데이터 소스 및 수집 범위
- **타겟 사이트**:
    - 커뮤니티 핫딜 게시판(현재는 3개의 커뮤니티)
- **수집 규모**: 실행 당 약 **600~700개** 아이템.
- **수집 주기**: 매일 2회 (00:00, 12:00 KST).
- **수집 범위**: 실행 시점 기준, **지난 24시간** 이내에 작성된 모든 게시글 (데이터 누락 방지).
- **수집 데이터**:
    - **제목 (Title)**: 핫딜 제목.
    - **링크 (Link)**: 커뮤니티 게시글 링크가 아닌 **실제 판매처 상품 링크**
    - **가격 (Price)**: 제목 또는 본문에서 추출.
    - **반응 (Engagement)**: 추천 수, 댓글, 댓글 수.
    - **날짜 (Date)**: 게시 시간.

## 2. 필터링 및 분석 파이프라인 (Filtering & Analysis Pipeline)
AI 비용 절감을 위해 3단계 깔때기(Funnel) 아키텍처를 사용합니다.

```mermaid
graph TD
    A[Raw Data Collection] -->|600+ Items| B(Hard Filter)
    B -->|Rules (Keywords/Comments)| C(Soft Scoring)
    C -->|Price Check (Naver API)| D{Score >= Threshold?}
    D -- No --> E[Drop (Cost $0)]
    D -- Yes --> F[AI Analysis]
    
    subgraph Core AI Logic
    F -->|Batch Request| G[Gemini 2.5 LLM]
    G -->|Context + Sentiment| H{Is Hotdeal?}
    end
    
    H -- Yes --> I[DB Save (HOT)]
    H -- No --> E
```

### 1단계: 하드 필터 (Hard Filter) - 규칙 기반 즉시 삭제
**다음 조건에 해당하면 비용 발생 없이 즉시 삭제됩니다 (Cost: $0):**
1.  **상태 키워드**: "종료", "품절", "매진", "취소".
2.  **유형 키워드**: "광고", "제휴", "체험단".
3.  **반응 미달**: **댓글 수 3개 미만**.
    - **예외 처리 (Time Decay)**: 작성 30분 미만의 신규 글은 댓글이 1개만 있어도 삭제하지 않습니다.
4.  **가격 정보 없음**: 제목에 유효한 가격 패턴이 없음.

### 2단계: 소프트 스코어링 (Soft Scoring) - 가산점 부여
**단순 필터링이 아닌 점수제(Score)를 도입하여 AI 분석 대상을 선별합니다.**
   - **생필품 키워드**: 식품, 세제, 휴지 등 생필품 관련 키워드 가산점.
   - **반응 속도 (Velocity)**: `(댓글수+1) / (경과시간+10)^1.5` 공식을 사용, 게시 직후 빠르게 반응이 오는 글에 높은 가산점 부여.
   - **가격 검증 (Price Check)**:
        - `네이버쇼핑 최저가`(naver_price) 자동 조회.
        - **하드 드롭**: `할인가 > 최저가` (최저가보다 비쌈) -> 즉시 탈락.
        - **가산점**: `할인가 < 최저가 * 0.85` (15% 이상 저렴) -> 가산점 부여 (+3점).
        - **절약 금액**: `최저가 - 할인가` 계산하여 AI 프롬프트에 제공.

### [핵심] 3단계: AI 심층 분석 (Gemini 2.5 Flash Lite)
**최종 선별된 "READY" 상태의 항목(약 150~300개)에 대해서만 LLM이 투입됩니다.**
- *참고: 24시간 수집 기준, 전체 수집량의 약 30~40%가 이 단계에 도달합니다.*

- **Input (AI에게 제공되는 정보)**:
    - `제목`: 상품명 및 구성.
    - `가격 정보`: 할인가, 네이버 최저가, **절약 금액(Savings)**.
    - `사용자 반응`: **댓글 원문 5개 (Sentiment Source)**, 추천 수, 반응 속도 점수.
- **Processing (AI의 역할)**:
    - **감성 분석**: "쟁여둔다", "역대가", "탑승" 등 긍정적 표현 식별.
    - **카테고리 분류**: 생필품(Food, Drink, Toiletries, Office) 여부 판독.
    - **가치 판단**: 가격 메리트와 사용자 반응을 종합하여 "진짜 핫딜"인지 판단.
- **Output (결과물)**:
    - `Status`: **HOT** / DROP / MAYBE
    - `AI Summary`: **쇼핑 호스트 톤**의 3줄 요약 큐레이션 (예: "📢 역대급 가격! 놓치면 후회할 구성입니다.")
    - `Sentiment Score`: 댓글 반응 기반 **0~100점** 감성 점수.
    - `Tags`: `#가성비`, `#쟁여템`, `#사무실` 등 상황별 **AI 해시태그**.
    - `Category`: 명확한 카테고리 태그.

## 3. 저장 및 출력 시스템
-   **스마트 캐싱**: `hash(제목 + 링크)`를 키로 사용하되, **날짜(Date)**가 변경되면 캐시를 만료시켜 매일 새로운 딜을 놓치지 않고 수집합니다.
-   **DB 저장 (Supabase)**:
    -   `hotdeals`: 핫딜 정보 (AI 분석 데이터 포함).
    -   `crawl_stats`: 매 실행 시 수집/필터/저장 수치 및 **총 절약 금액(Total Savings)** 기록.
    -   **신고 시스템**: `report_count` 컬럼을 통해 프론트엔드 유저 신고 반영.
-   **스케줄러**: GitHub Actions를 통해 **매일 00:00, 12:00 KST**에 자동 실행됩니다.

## 4. 프로젝트 구조
```bash
.
├── app/                  # 핵심 애플리케이션 코드
│   ├── core/             # 분석기(Analyzer), DB, 설정 등 공통 로직
│   ├── crawlers/         # 사이트별 크롤러 (Ppomppu, FMKorea, Arca)
│   └── models/           # 데이터 모델 (Deal)
├── sql/                  # DB 스키마 및 마이그레이션 SQL 파일들
├── .github/workflows/    # 자동화 스케줄 설정 (daily_crawl.yml)
└── run_once.py           # [Main] 파이프라인 실행 진입점
```
